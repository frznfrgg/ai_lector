(0.0, 12.82):  Так, друзья, ну все, я больше подписывать пока ничего не буду, сейчас будет лекция. 
(13.82, 18.74):  Значит, смотрите, ну я из года в год читаю, конечно, эти лекции, в принципе, есть их записи, 
(19.28, 24.12):  но я с удовольствием прочитаю снова, может быть, что-нибудь новое случится на этот раз. 
(24.78, 28.94):  На самом деле мне, конечно, хочется читать гораздо быстрее, чем обычно я это делаю, 
(28.94, 0.0):  
(3.64, 4.38):  то есть начитывать такой более объемный, более мощный материал. 
(11.08, 11.6):  Но традиция сложилась такая, что поскольку к магистратуру идут люди самого разного, 
(15.8, 16.58):  начального уровня подготовки, приходится очень многое напоминать. 
(19.06, 19.48):  Ну и, наверное, сейчас мы тоже что-то такое проделаем. 
(24.02, 0.0):  Но все-таки я хочу с места в карьер пойти на какую-то достаточно продвинутую тематику. 
(9.76, 10.3):  Значит, курс посвящен комбинаторике, но методы, которые хочется рассказывать в рамках этого курса, они достаточно продвинутые. 
(17.02, 17.52):  То есть я ни в коем случае не хочу читать такой вводный курс о том, что такое C из НПК, там, не дай бог. 
(20.66, 21.26):  Я предполагаю, что слушатели знают, что такое C из НПК. 
(24.3, 24.9):  Я предполагаю, что слушатели, конечно, знают, что такое граф. 
(0.62, 5.68):  поэтому с места в карьер я буду рассказывать какие-то вещи типа графы, гиперграфы, 
(6.12, 9.86):  но все это будет собрано в рамках методов. 
(10.42, 14.6):  То есть я буду рассказывать про методы, которые работают в современной комбинаторике. 
(15.2, 17.52):  В первую очередь это будет вероятностный метод, 
(18.06, 21.7):  ну а попозже мы придем еще к алгебраическим каким-то технологиям, 
(21.7, 25.88):  которые тоже позволяют решать сложные комбинаторные задачи. 
(26.16, 29.04):  Ну и все будет построено вокруг некоторого количества задач, 
(29.14, 0.0):  
(3.34, 3.88):  такой вот, если угодно, экстремальной комбинаторики. 
(9.34, 9.4):  Экстремальной не в смысле сложности ее, а в смысле, что обычно изучаются задачи на экстремумы, 
(14.54, 15.64):  то есть найти максимальное количество чего-нибудь или там самый какой-то такой-то случай. 
(17.52, 18.32):  Вот, ну сейчас будут конкретные примеры. 
(21.52, 22.08):  Давайте действительно начнем с вероятностного метода. 
(24.94, 0.0):  Его в каком-то смысле будем развивать очень последовательно, 
(9.82, 10.36):  то есть начнем с самых простых моментов, а потом перейдем в конечном счете к очень продвинутым неравенствам плотной концентрации меры и еще чего-то. 
(15.82, 16.68):  Ну, то есть кого-то может кокнуть, но вы магистранты, все-таки, пожалуйста, держитесь. 
(21.8, 22.98):  Соответственно, если вы чувствуете, что вас кокает сейчас, вы что-то не понимаете, вы, конечно, обязательно спрашивайте. 
(26.34, 26.84):  Тут, я так понимаю, что можно и голосом говорить, да, ведь в Зуме? 
(0.0, 8.08):  народу там всего четыре человека кажись или три или они всех вижу всех то есть 
(8.08, 13.58):  народу совсем немного мы можем очень так келей на пообщаться зато всем все будет 
(13.58, 20.96):  понятно если вы не будете стесняться задавать вопросы сейчас я еще мил найду 
(24.74, 0.0):  
(11.98, 12.44):  Так, ну давайте я сразу прямо скажу, что есть такой замечательный объект в комбинаторике, в теории графов, 
(15.76, 16.46):  который обобщает понятие графа и называется он гиперграф. 
(19.42, 23.96):  Это будет удобный объект для самых разных задач, и вокруг него как раз будут формулироваться интересные такие экстремальные вопросы. 
(24.8, 29.76):  Ну и вероятностный метод на них тоже можно будет очень хорошо иллюстрировать. 
(0.0, 11.06):  Еще раз. Мы начинаем изучать вероятностный метод в комбинаторике с постановок некоторых конкретных задач из простейших вероятностных методов. 
(11.06, 15.92):  Для этого, в частности, мне нужно напомнить, что есть такой объект, который называется гиперграф. 
(17.5, 0.0):  
(21.44, 22.78):  Ну, подобно графу, его можно обозначить, естественно, какой-нибудь буквой, у него есть, так же точно, как и у графа, некоторое множество вершин, конечное множество, как правило, конечное множество объектов любых, совершенно любой природы, и есть множество ребер. 
(0.0, 9.4):  Ну, вроде как, вообще звучит никакой разницы, да, граф у него есть вершины, есть ребра, и у гиперграфа тоже есть вершины и ребра. 
(10.3, 22.36):  Но гиперность, так сказать, обобщения состоит в том, что ребра в данном случае это не обязательно пара элементов, а это какие-то подмножества, множество вершин. 
(22.36, 28.92):  Не обязательно пара вершин, но может быть тройки вершин, например, или какие-то наборы большей мощности. 
(29.06, 0.0):  
(7.0, 9.24):  То есть Е можно вот так написать, это подмножество в множестве всех подмножеств. 
(13.28, 13.66):  Так, 2 в степени В это естественное обозначение для множества всех подмножеств. 
(18.16, 18.44):  Если мы берем там подмножество, значит такая совокупность множеств вершин. 
(19.82, 21.16):  Совокупность множеств вершин. 
(26.78, 0.0):  Так, если вопросов нет, я, конечно, могу разогнаться, я с любой скоростью могу рассказывать. 
(5.28, 6.38):  Не знаю, опять же, какая подготовка начальная у тех, кто сейчас меня слушает. 
(9.08, 9.94):  Вот, гиперграф называется N-однородным. 
(13.04, 14.02):  Мы будем изучать только однородные гиперграфы. 
(25.68, 0.0):  Называется N-однородным, если любое его ребро состоит из N вершин, имеет мощность N. 
(4.66, 5.6):  Ну, то есть два однородных гиперграфа – это обычный граф. 
(10.46, 11.04):  Ну, мы будем считать, что n больше либо равняется двойке, 
(17.5, 19.62):  потому что, как правило, по крайней мере, поначалу нас будут интересовать раскраски вершин гиперграфа в духе того, как красят вершину графов. 
(19.74, 24.14):  Ну, тут такая важная тематика, связанная с раскраской. 
(27.34, 0.0):  
(11.58, 20.7):  так ну дать я все-таки задам вопрос слушателям не знаю как-нибудь скажите мне пожалуйста вот у графа что такое хроматическое число вы знаете так вот сидящего аудитории человек знает а сидящие 
(20.7, 0.0):  
(12.66, 20.56):  в зуме тоже знаем один знает а еще двое да так ну прекрасно значит хроматическое число графа вы знаете это минимальное число цветов такой раскраски вершин что каждое ребро имеет концы разного цвета 
(20.56, 0.0):  
(12.0, 17.3):  если у нас теперь есть гипер граф ну например n однородный и n больше двойки строго то то понятие хроматического числа в принципе можно обобщить по-разному можно сказать что раскраска 
(17.3, 23.12):  правильная если раскраска вершинка что опять вершины краситься правильным образом если 
(23.12, 0.0):  
(8.28, 13.68):  например все вообще вершины каждого ребра разного цвета ну это будет естественным обобщением того как мы понимали хроматическое число графа концы каждого ребра имеют разные цвета но тут концов 
(13.68, 21.04):  больше у каждого ребра можно сказать что вот каждый конец там все вот эти концы имеют разные цвета на 
(21.04, 27.36):  самом деле это такое ну не совсем правильное в каком-то смысле обобщение более правильное 
(27.36, 0.0):  
(10.94, 0.0):  обобщение звучит в каком-то смысле проще. Хроматическое число гиперграфа – это минимальное 
(21.34, 0.0):  число цветов, в такой раскраске множество вершин, что каждое 
(16.74, 23.46):  ребро просто неодноцветно. То есть мы не требуем, чтобы все вершины данного ребра имели разные цвета. Мы хотим только, чтобы там присутствовало хотя бы два цвета среди цветов вершин. Каждое 
(23.46, 28.4):  ребро должно быть неодноцветным. Тогда мы говорим, что все хорошо. Наименьшее количество цветов 
(28.4, 0.0):  
(6.36, 17.52):  такой раскраски называется карматическим число гипер граф такой на самом деле в каком-то смысле более естественное обобщение так ну давайте наверное я докажу сразу очень простую теорему 
(17.52, 0.0):  
(12.08, 12.56):  которую придумал, придумали даже, вернее, Эрдош и Хайнал, ну, как придумали, пронаблюдали. 
(16.54, 23.96):  Простое совершенно утверждение, понятно, ничего тут такого открытия сейчас не будет. Эрдош и Хайнал пронаблюдали ее в 1961 году, но она послужила затравкой для очень большой науки 
(23.96, 0.0):  
(6.0, 7.02):  и в частности для очень существенных применений вероятностного метода в комбинаторных задачах. 
(8.42, 9.48):  Значит, теорема звучит так. 
(16.6, 23.22):  Пусть H, N однородный гиперграф, 
(0.0, 16.38):  Пусть также количество его ребер, ну давайте строго меньше, чем 2 в n-1 степени 
(16.38, 20.56):  n и n одно и то же, здесь и здесь, то есть она однородный гиперграф 
(20.56, 25.84):  А количество ребер в терминах вот этого n, количество вершин в каждом ребре, не слишком большое 
(25.84, 29.96):  Меньше, чем 2 в n-1 степени 
(29.96, 0.0):  
(9.34, 12.3):  тогда хиотаж не больше двойки. 
(15.4, 15.46):  Ну, на самом деле можно сказать, равняется двойке, 
(17.14, 17.76):  потому что если есть хотя бы одно ребро, 
(19.8, 20.96):  то, конечно, одного цвета уже не хватит. 
(22.72, 24.02):  Если гиперграф не пустой, 
(25.86, 25.94):  но формально, наверное, это правильнее, 
(27.24, 27.56):  потому что вдруг он пустой, 
(0.0, 2.98):  Вот вдруг такое несчастье случилось. 
(4.06, 13.06):  Вот. А, наверное, присутствующие в этой как живой, так и виртуальной аудитории еще знают такое выражение, как двудольный граф. 
(14.5, 18.64):  Но вот это естественное обобщение, потому что что такое двудольный граф? 
(19.1, 21.36):  Это граф, у которого хроматическое число 2. 
(22.4, 27.0):  Если у гиперграфа хроматическое число 2, ну, тоже, естественно, его называют двудольным. 
(27.0, 0.0):  
(8.2, 8.26):  Здесь две доли вершин, покрашенные в свои цвета, и все ребра перекрестно накрываются этими вершинами. 
(12.14, 13.4):  А внутри этого множества вершин одного цвета никаких ребер нет. 
(20.54, 21.48):  То есть, если ребер у гиперграфа относительно мало, то он двудолин. 
(28.28, 0.0):  Но доказательства можно, конечно, провести, не прибегая к помощи какой-либо вероятности, 
(16.08, 0.0):  Потому что здесь вероятность элементарная. Ну, то есть можно сравнивать просто количество раскрасок. И это будет то же самое абсолютно. Если вы такие умные, продвинутые, уже заранее понимаете такие вещи, это здорово. 
(13.98, 14.46):  Но я вас все равно предвосхищаю, так сказать, ваше замечание предвосхищаю и говорю, что да, конечно, в данном случае в доказательстве вероятность будет не очень по существу, но это затравка для будущего продвинутого вероятностного метода. 
(0.0, 13.18):  Значит, смотрите, давайте я все-таки буду рассуждать в терминах вероятности, что опять же я предполагаю, что слушатели, которые пришли в магистратуру, базовый курс вероятности знают. 
(13.62, 26.7):  Ну, например, товарищи, тем не менее я вас спрошу, вот вы знаете, что такое схема испытаний Бернули? Монетку когда бросают, да? Ну, там в Зуме тоже хорошо бы как-то отреагировать. 
(0.0, 4.4):  По-моему, народ не то, чтобы полег, а просто отвлекся. 
(6.36, 7.08):  Да, знаем. 
(7.36, 12.16):  А? Знаете, если бы монетку бросать, там схема испытаний Бернули возникает, 
(12.24, 14.74):  пока никаких продвинутых вопросов тут нет. 
(14.74, 18.5):  Давайте просто возьмем, зафиксируем этот гиперграф 
(18.5, 23.66):  и будем красить вершины независимо друг от друга, 
(23.78, 26.72):  то есть взаимно независимо, независимо в совокупности, 
(26.82, 0.0):  
(8.04, 18.18):  каждую с вероятностью 1 2 условно в красный цвет и с вероятностью 1 2 в синий красим вершины одну за другой или одновременно просто взаимно независимо в красный и синий цвета с вероятностью 
(18.18, 26.82):  1 2 ну фактически я просто мог сказать давайте рассмотрим случайную раскраску то есть каждая 
(26.82, 0.0):  
(6.84, 12.18):  раскраска имеет вероятность единица поделить на 2 в степени мощность множества вершин но мне не хочется так длинно говорить потому что я даже не знаю сколько вершин заметьте в этом утверждении 
(12.18, 19.44):  количество вершин не играет никакой роли играет роль только количество вершин в каждом ребре но 
(19.44, 24.32):  количество всех вершин не играет роль никакой поэтому мне удобнее говорить вот именно в 
(24.32, 0.0):  
(1.58, 2.04):  в терминах бросания монетки. 
(3.82, 3.9):  Красим вершины в красные и синие цвета, 
(5.12, 6.92):  с вероятностью одна вторая. 
(10.64, 10.76):  Так, ну, вам, конечно, стоит вспоминать немножко вероятности, 
(13.68, 15.3):  понимать, что, по сути, все события теперь – это просто множество раскрасок, правильно? 
(16.78, 18.76):  Потому что элементарный исход – это раскраска. 
(19.96, 22.18):  Ну, так, чтобы вам просто было легче ориентироваться, 
(22.18, 24.66):  а так-то ничего особо умного нет. 
(25.42, 27.6):  Вот смотрите, если у нас есть какое-то ребро, 
(29.48, 0.0):  
(10.36, 11.66):  Ну, давайте поставим ему в соответствие событие, состоящее в том, что Е одноцветно вот в такой случайной раскраске. 
(17.08, 18.46):  А с индексом Е это событие, состоящее в том, что в случайной раскраске ребро одноцветно. 
(22.0, 23.66):  Ну, грандиозный вопрос, какова вероятность этого события. 
(28.54, 0.0):  Вот опять, здесь очень удобная интерпретация с монеткой, потому что нам не нужно знать, сколько вершин, 
(4.18, 8.44):  для того, чтобы сразу сказать, какова вероятность этого события. 
(9.88, 10.52):  Ну да, да. 
(14.28, 15.46):  2 умножить на 1 вторая в n-ной степени. 
(20.06, 20.54):  2, потому что оно все может быть красным, оно все может быть синим, вот два варианта. 
(23.3, 23.66):  Ну и для каждого из вариантов вероятность 1 вторая в n. 
(27.08, 0.0):  Все n вершин данного ребра покрасились в один и тот же цвет. 
(4.96, 5.48):  Ну, это можно вот так переписать, после чего, по-моему, все становится понятно. 
(9.24, 9.78):  Но я все-таки напишу с большим пафосом то, что получилось. 
(15.26, 15.66):  Вот у нас есть объединение по всем Е из Е событий АЕ. 
(19.42, 24.02):  Что из себя представляет вот это событие, состоящее в объединении? 
(27.48, 29.48):  Нет, это то, что хотя бы одно одноцветно. 
(0.0, 12.42):  АЕ – это что вот это конкретное ребро одноцветное, а объединение АЕ – это что хотя бы одно ребро целиком одноцветное. 
(12.98, 14.78):  То есть это плохое событие для нас. 
(15.62, 22.12):  Нам хочется, чтобы все ребра были не одноцветные, а это плохое событие, что существует одноцветное ребро. 
(23.88, 28.0):  Это множество раскрасок, в которых хотя бы одно ребро одноцветное. 
(29.0, 0.0):  
(13.24, 22.36):  цвет ну естественно это не больше чем сумма по е е вероятности а.е. а это в точности равно 2 в степени 1 минус n на мощности е но мощность е по условию меньше чем 2 в степени минус 1 строго 
(22.36, 0.0):  
(9.56, 19.06):  поэтому мы получаем денечку нет ну конечно любая вероятность не превосходит единицы тут ничего умного нет но здесь-то строго меньше и это плохое событие существует одноцветное ребро значит 
(19.06, 24.78):  отрицание этого плохого события действительно такое как нам нужно все ребра не одноцветны и 
(24.78, 0.0):  
(3.12, 4.42):  И его вероятность строго положительна. 
(5.84, 6.6):  Ну давайте я напишу, ладно. 
(12.78, 14.36):  Вероятность пересечения отрицаний стала быть строго больше нуля. 
(17.84, 18.74):  Но вы не забывайте, это пересечение отрицаний – это просто множество раскрасок. 
(21.78, 23.22):  Ну то есть это множество очевидно не пусто, если вероятность больше нуля. 
(23.4, 23.48):  Все. Все. 
(28.62, 0.0):  
(10.48, 14.84):  Все это можно было, как я уже говорил, произнести на количественном языке Ну просто сказать, есть два в степени мощность V раскрасок 
(14.84, 19.64):  Среди них есть такие, которые оставляют одноцветным первое ребро 
(19.64, 22.36):  Такие, которые оставляют одноцветным второе 
(22.36, 24.2):  Вот мы их объединим 
(24.2, 28.74):  Мощность этого множества будет меньше, чем 2 в степени V 
(28.74, 0.0):  
(17.78, 0.0):  Ну вот, значит, есть такие раскраски, которые действительно, вот, какие нам нужны. Но так гораздо удобнее. И главное, что потом начнутся какие-то более продвинутые вероятностные технологии, повязанные, скажем, на независимость событий, повязанные на неравенство концентрации меры. 
(6.64, 7.54):  Ну, может, вы таких страшных слов не слышали, но неравенство Чебышева вы же слышали? 
(9.76, 9.84):  А это неравенство плотной концентрации мира. 
(12.32, 12.38):  Но я про это потом расскажу очень подробно. 
(13.82, 13.88):  Есть более тонкие неравенства. 
(14.4, 14.98):  Это важно. 
(16.8, 16.88):  Это используется не только в комбинаторике. 
(18.58, 18.66):  Это нужно для вероятности, для самой. 
(20.98, 22.14):  Это нужно для оптимизации, например. 
(22.38, 22.44):  Вот. 
(22.7, 22.88):  Ну, все. 
(24.7, 0.0):  Теорему так или иначе я доказал. 
(5.8, 6.52):  и она мотивирует появление некоторой экстремальной характеристики на множестве гиперграфов, 
(8.24, 10.62):  которая обозначается вот так. 
(14.54, 15.94):  Собственно, это обозначение Эрда Шхайнелл и придумали в 1961 году. 
(20.08, 22.52):  Они, правда, почему-то употребляли термин «свойство B гиперграфа», вместо того, чтобы говорить о хроматическом числе. 
(22.64, 24.22):  Ну, как-то так вот тогда сложилось. 
(24.3, 24.84):  Но неважно. 
(25.34, 27.92):  Даже не буду говорить, что такое свойство B не имеет значения. 
(27.92, 0.0):  
(18.68, 25.68):  M от N – это минимальное натуральное M, такое, что существует N-однородный гиперграф, 
(0.0, 9.66):  у которого m ребер, 
(12.5, 14.58):  а хроматическое число больше 2. 
(16.68, 17.5):  Вот так. 
(21.54, 23.92):  Ну, давайте, это надо осознать. 
(24.04, 24.8):  Мы что поняли? 
(24.88, 28.08):  Если у гиперграфа ребер меньше, чем 2 в n-1 степени, 
(28.54, 29.48):  то это не так. 
(0.0, 1.78):  У него хроматическое число не больше 2. 
(3.28, 7.84):  Ну, то есть это означает, что здесь мы пытаемся найти минимальное m, 
(7.92, 11.96):  при котором существует контрпример к теореме, только что доказанной. 
(13.06, 15.8):  Очевидно, оно больше, чем 2 в n минус 1, больше либо равно. 
(16.62, 21.68):  То есть теорема, которую мы доказали, влечет в утверждение о том, 
(21.8, 26.72):  что m от n больше либо равно 2 в n минус 1 степени. 
(27.1, 0.0):  
(6.32, 6.68):  Ну еще раз, потому что если у гиперграфа меньше, чем столько ребер, то он красится в два цвета. 
(13.0, 15.12):  А нам нужно такое количество ребер, при которых найдется гиперграф, который невозможно покрасить в два цвета. 
(15.88, 18.42):  Понятно говорю, да? 
(20.22, 21.8):  Ну в зуме народ тихий-тихий. 
(22.8, 23.88):  Ну наверное понятно. 
(24.22, 25.22):  Ну да. 
(26.18, 0.0):  Ну хорошо. 
(8.16, 9.04):  Но давайте подумаем, без всякой даже вероятности, может быть вообще все глупость какая-то, мы некорректную задачу поставили. 
(11.16, 15.36):  Можно как-то МОТН сверху оценить. 
(17.56, 19.34):  Ну, задача может оказаться глупой. 
(26.3, 0.0):  Вдруг не существует контрпримера ни с каким количеством ребер, а вообще на самом деле просто любой Н-однородный гиперграф красится. 
(3.62, 4.16):  Во-во-во, у меня в аудитории уже появилась очень правильная мысль. 
(6.34, 6.42):  Я говорю, что это обычно придумывается самостоятельно. 
(8.78, 9.5):  Полный, так, какой-нибудь, какой. 
(12.9, 16.06):  Но надо поменьше, конечно, получше оценку получить. 
(17.54, 22.06):  Полный – это очень хорошая идея. 
(23.72, 24.32):  Но даже 2n-1. 
(27.5, 0.0):  Правильно, 2n, конечно, хватит, но даже 2n-1 хватит. 
(5.1, 14.4):  значит что мы тут обсудили с единственным слушателем который расположен 115 кпм и на может меня два слушателя конечно один из них оператор я не знаю вот давайте рассмотрим 
(14.4, 21.72):  гипер графу которого вот такие вершины так ну товарищ я думаю что понятно вершины можно просто 
(21.72, 27.24):  всегда считать числами это неважно просто как ты их там перенумеровали и все люди это там или еще 
(27.24, 0.0):  
(6.78, 17.04):  что это уже не другой вопрос и числа от единицы до 2 n минус 1 и мы вот на этом множестве вершин это в берем все возможные ребра то есть е можно вот так обозначить этот сейс в паян это тоже для 
(17.04, 23.28):  комбинаторе кстати стандартное обозначение когда c берут из множества они из его мощности то есть 
(23.28, 29.76):  это не число а множество всех n элементных подмножеств вот этого множества до стандартное 
(29.76, 0.0):  
(3.84, 6.88):  обозначение. C из множества почему-то это все возможные подмножества. Мощность его, конечно, такая, C из 
(6.88, 11.2):  мощности Vπn. Берем такой полный n-однородный гиперграф 
(11.2, 14.36):  на этом множестве вершин. Но смотрите, как бы мы не 
(14.36, 18.12):  покрасили множество вершин в два цвета, в какой-то цвет 
(18.12, 21.48):  из этих двух будут покрашены не менее чем n вершин. 
(21.98, 28.54):  По очевидному принципу Дерихле. Поскольку каждое 
(28.54, 0.0):  
(2.04, 4.06):  множеством мощности N сейчас является ребром, ну вот это уже 
(4.06, 6.02):  противоречие. Покрасили 
(6.02, 7.98):  в два цвета и тотчас же нашли 
(7.98, 9.86):  какое-то одноцветное ребро. При 
(9.86, 11.94):  любой раскраске. То есть как не 
(11.94, 13.96):  старайся, как не крась, но вот какое-то ребро 
(13.96, 15.98):  будет целиком, к сожалению, одного цвета. 
(16.66, 18.02):  И это означает, 
(18.16, 19.98):  что M от N не превосходит 
(19.98, 21.4):  C из 2N-1 
(21.4, 22.46):  ОН. 
(29.4, 0.0):  
(11.2, 11.82):  Ну, товарищи, если вы у меня прям продвинутые-продвинутые, 
(14.5, 18.46):  то вы даже, наверное, знаете, как растет вот эта величина. 
(22.2, 23.44):  Нет, ну я чуть-чуть иронизирую, потому что на самом деле это не то, что великое знание. 
(23.66, 24.6):  Как? 
(26.62, 28.2):  Нет, n в степени n – это чересчур. 
(0.0, 10.38):  Это превосходит всяческие ожидания, потому что, очевидно, следующее, понял, в этом месте уже не понимаете, но ничего страшного. 
(11.72, 12.84):  Вот так вот напишем. 
(19.56, 23.36):  Ну, сумма всех цешек с фиксированным нижним индексом это что? 
(26.92, 28.16):  Да, в степени n. 
(28.16, 0.0):  
(11.42, 20.38):  2 в степени 2n минус 1, ну примерно 4 в n, в точности, деление на двойку. Вот, но я специально в серединке написал нашу c, ну во-первых, потому что она действительно в серединке, то есть она самая большая. 
(21.24, 23.78):  Вы знаете, что они возрастают сначала, а потом убывают. 
(24.28, 0.0):  
(16.82, 0.0):  Любовь и вероятность. Знаете, вы знаете вот такой горбик, называется нормальное распределение. А он откуда берется? Он ровно отсюда берется. Слышите, сначала вот так растут, растут, а потом так же симметрично убывают. Ну ладно, это какие-то страшные слова, нормальное распределение. 
(2.8, 3.28):  Это школьнику можно объяснить, который комбинаторику знает. 
(8.38, 8.48):  Понятно, что сумма положительных слагаемых, в серединке стоит какое-то из них, 
(10.46, 12.7):  но ясно, что оно меньше, чем вот эта величина. 
(16.78, 17.22):  То есть это точно тупо меньше, чем 4 в степени n. 
(18.78, 20.22):  Ну или 4 в степени n пополам. 
(23.74, 24.74):  Другое дело, что поскольку это самое большое слагаемое, 
(29.24, 0.0):  то оно же, конечно, и больше. 
(5.26, 7.7):  Больше оно, чем 2 в степени 2n-1 поделить на 2n. 
(10.08, 10.4):  Где 2n – это количество слагаемых. 
(13.82, 15.18):  Оно самое большое и всего слагаемых 2n. 
(17.82, 18.56):  Ну, значит, оно больше, чем вся сумма поделить на 2n. 
(21.6, 21.8):  Но вы, как люди, поступившие в магистратуру, понимаете, конечно, 
(26.88, 27.38):  что экспоненты 4 в n существенно значимее для вечности, 
(29.46, 0.0):  чем какая-то там линейно растущая функция. 
(6.62, 6.76):  Поэтому в каком-то смысле можно считать, что верхняя оценка имеет порядок роста 4 вен. 
(7.88, 8.94):  Ну, в каком-то смысле. 
(11.82, 14.04):  А нижняя оценка 2 вен, то есть засор-то большой. 
(16.22, 21.4):  Но возникает вопрос, нельзя ли его устранить. И вот вопрос об устранении этого зазора решается в том числе с помощью методов, 
(21.76, 25.68):  которые развивают вот этот вот простейший вероятностный подход. 
(26.3, 29.32):  То есть с одной стороны мы сможем сейчас продвинуться в этой задаче, 
(29.44, 0.0):  
(4.86, 5.2):  а с другой стороны мы потихоньку начнем двигаться в сторону чуть более продвинутых методов, 
(7.86, 9.64):  нежели просто вероятность объединения не больше, чем сумма. 
(10.5, 11.96):  Я понятно говорю? 
(16.46, 17.02):  Так, ну, я сделаю пару маленьких чисто технических замечаний. 
(20.78, 21.98):  На самом деле, вот дайте посмотрим на это доказательство. 
(24.86, 29.88):  Вот здесь вот можно убрать равно. Но плохо. Пачка. 
(0.0, 1.22):  Можно убрать равно. 
(3.36, 4.0):  Понимаете, почему? 
(5.86, 10.94):  Ну, потому что в этом объединении есть, как сказать, пересечение всех АЕ, 
(11.42, 15.84):  не пустое, есть идиотская раскраска, все вершины покрасили в красный цвет. 
(16.54, 19.38):  Она принадлежит всем вот этим событиям. 
(19.8, 23.18):  Ну, есть и другая идиотская раскраска, все вершины покрашены в синий цвет, 
(23.18, 25.78):  и она тоже принадлежит вот этим всем АЕ. 
(26.26, 0.0):  
(9.04, 13.52):  То есть пересечение не пустое, но это значит, что можно такой, не знаю, идиотский вариант формулы включения-исключения применить и будет строго меньше. 
(19.1, 19.44):  Ну, это означает, что m от n можно увеличить на единичку. 
(0.0, 11.38):  В свете того, что я только что говорил, да мне плевать над вайном знаменателе, это, конечно, кажется каким-то совсем изыском, но мне на самом деле не плевать над вайном знаменателе совершенно. 
(12.32, 22.34):  И вы сейчас увидите, что поскольку М-атенда растет ближе к этому, то там начнутся уже вопросы о том, как вот такие величины такого порядка учитывать. 
(23.28, 0.0):  
(10.74, 11.7):  Пока, когда зазор в экспоненту раз, ну, говорить не о чем, но если сейчас появятся какие-то более содержательные соображения, то мы станем и такие вещи тоже отлавливать. 
(13.1, 14.82):  Вот, ну все. 
(15.42, 17.0):  Ну все. 
(26.6, 0.0):  Я думаю, что за оставшееся время, у нас его не так много, ну час у нас остался где-то, да, я попробую рассказать про улучшение верхней оценки. 
(9.0, 12.18):  И здесь уже появится чуть более продвинутое вероятностное соображение, которое состоит в какой-то дополнительной независимости событий. 
(18.42, 29.0):  Так, сейчас мне нужно изыскать тряпку. Я ее ношу. 
(0.0, 11.82):  Интересная доска. 
(12.76, 18.22):  Когда начинаешь стирать мокрой тряпкой, вдруг обнаруживается, что там такая клетчатая сеточка. 
(0.0, 29.98):  Продолжение следует... 
